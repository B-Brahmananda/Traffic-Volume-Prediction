{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7orIedGckTt8","executionInfo":{"status":"ok","timestamp":1735276134414,"user_tz":-330,"elapsed":3499,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}}},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import sklearn"]},{"cell_type":"markdown","metadata":{"id":"nmmr2_EpuuRL"},"source":["### Pre-processing"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"m48BNoQUk9x-","executionInfo":{"status":"ok","timestamp":1735276137453,"user_tz":-330,"elapsed":711,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}}},"outputs":[],"source":["# Reading the Training Data\n","df = pd.read_csv(\"/content/train_set_dirty.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nc31ShP2aTYx"},"outputs":[],"source":["# 1. Displaying the first 10 records\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxeR34vpaTYx"},"outputs":[],"source":["# 2. Check dataset info - field types, non-null values, dtypes\n","df.info()\n","n_features = df.shape[1]\n","print(n_features)\n","feature_names = df.columns\n","print(feature_names)\n","print(\"Null counts for each column:\")\n","print(df.isnull().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bK2ET3svaTYx"},"outputs":[],"source":["# 3a. Pre-processing (Cleaning): Address missing (NULL) values - drop or imputation\n","# Check number and percentage of missing values\n","def check_missing_values(df):\n","    missing_values = df.isnull().sum()\n","    missing_percentage = (df.isnull().sum() / len(df)) * 100\n","    missing_df = pd.DataFrame({\n","        'Missing Values': missing_values,\n","        'Percentage': missing_percentage.round(2)\n","    })\n","    print(\"Missing Values Analysis:\")\n","    print(missing_df[missing_df['Missing Values'] > 0])\n","\n","check_missing_values(df)"]},{"cell_type":"code","source":["# Check for exact duplicates (all columns must match exactly)\n","duplicates = df[df.duplicated(keep='first', subset=df.columns)]\n","\n","'''\n","# Or check duplicates based on specific columns that matter most\n","important_columns = ['traffic_volume', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'date_time', 'weather_description']\n","duplicates = df[df.duplicated(subset=important_columns, keep='first')]\n","'''\n","\n","# To see the actual duplicate rows along with their original rows\n","df[df.duplicated(keep=False)].sort_index()\n","\n","print(\"Original shape:\", df.shape)\n","df_no_duplicates = df.drop_duplicates()\n","print(\"Shape after dropping duplicates:\", df_no_duplicates.drop_duplicates().shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Qjyf7iemUSUf","executionInfo":{"status":"ok","timestamp":1735276153385,"user_tz":-330,"elapsed":475,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}},"outputId":"13353430-9147-4489-a10f-7935f17f8a4c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Original shape: (38563, 9)\n","Shape after dropping duplicates: (38550, 9)\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZorZAEL0aTYy","executionInfo":{"status":"ok","timestamp":1735276156836,"user_tz":-330,"elapsed":685,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}}},"outputs":[],"source":["# 3d. Pre-processing (Encoding): Convert categorical values to numeric\n","# Use df['col_name'].value_counts() to find out all the categories available per column"]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","def prepare_features(df_no_duplicates):\n","    # Create a copy to avoid modifying original data\n","    df_processed = df_no_duplicates.copy()\n","\n","    # Handle datetime\n","    df_processed['date_time'] = pd.to_datetime(df_processed['date_time'])\n","    df_processed['hour'] = df_processed['date_time'].dt.hour\n","    df_processed['day'] = df_processed['date_time'].dt.day\n","    df_processed['month'] = df_processed['date_time'].dt.month\n","    df_processed['day_of_week'] = df_processed['date_time'].dt.dayofweek\n","\n","    # Handle categorical variables\n","    # Option 1: Label Encoding\n","    le = LabelEncoder()\n","    categorical_columns = ['weather_main', 'weather_description', 'holiday']\n","\n","    for col in categorical_columns:\n","        df_processed[f'{col}_encoded'] = le.fit_transform(df_processed[col].fillna('Missing'))\n","\n","    # Option 2: One-Hot Encoding\n","    #df_processed = pd.get_dummies(df_processed, columns=categorical_columns)\n","\n","    return df_processed\n","\n","# Apply the transformations\n","df_processed = prepare_features(df_no_duplicates)\n","\n","# Check new columns\n","#print(\"New numerical columns:\", df_processed.select_dtypes(include=['number']).columns.tolist())\n","#print(df_processed.info)\n","for column in df_processed.columns:\n","    print(column)\n"],"metadata":{"id":"RXRWL3_GXBrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukdMsP2zaTYy"},"outputs":[],"source":["# 4a. Data understanding - Find out stats regarding your data (df.describe(), df.mean(), df.median())\n","df_processed.info()\n","df_processed.head(10)"]},{"cell_type":"code","source":["# Drop original categorical columns since we have their encoded versions\n","columns_to_drop = ['holiday', 'weather_main', 'weather_description', 'date_time']\n","\n","# Create new dataframe without these columns\n","df_cleaned = df_processed.drop(columns=columns_to_drop)\n","\n","# Verify columns after dropping\n","print(\"Remaining columns:\", df_cleaned.columns.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"vP2DxilJ3JAa","executionInfo":{"status":"ok","timestamp":1735276169865,"user_tz":-330,"elapsed":670,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}},"outputId":"7be3f539-d182-40d7-b015-66d60515c33b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Remaining columns: ['traffic_volume', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'hour', 'day', 'month', 'day_of_week', 'weather_main_encoded', 'weather_description_encoded', 'holiday_encoded']\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_UfeJQTaTYz"},"outputs":[],"source":["# Any other relevant pre-processing (upto your exploration)\n","# Make a copy of the dataframe\n","df_clean = df_cleaned.copy()\n","\n","# For traffic_volume (target variable)\n","df_clean['traffic_volume'] = df_clean['traffic_volume'].fillna(df_clean['traffic_volume'].mean())\n","\n","# For temperature and other weather measurements\n","df_clean['temp'] = df_clean['temp'].fillna(df_clean['temp'].median())\n","df_clean['rain_1h'] = df_clean['rain_1h'].fillna(0)  # Assume no rain if NaN\n","df_clean['snow_1h'] = df_clean['snow_1h'].fillna(0)  # Assume no snow if NaN\n","df_clean['clouds_all'] = df_clean['clouds_all'].fillna(df_clean['clouds_all'].median())\n","\n","# For encoded categorical columns (if they have any NaNs)\n","df_clean['weather_main_encoded'] = df_clean['weather_main_encoded'].fillna(df_clean['weather_main_encoded'].mode()[0])\n","df_clean['weather_description_encoded'] = df_clean['weather_description_encoded'].fillna(df_clean['weather_description_encoded'].mode()[0])\n","df_clean['holiday_encoded'] = df_clean['holiday_encoded'].fillna(0)  # Assume not a holiday if NaN\n","\n","# For time-based features\n","time_columns = ['hour', 'day', 'month', 'day_of_week']\n","for col in time_columns:\n","    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n","\n","# Verify no NaN values remain\n","print(\"\\nRemaining NaN values after cleaning:\")\n","print(df_clean.isnull().sum())"]},{"cell_type":"code","source":["# Separate features (X) and target (y)\n","# First, create a copy of all features except traffic_volume\n","X = df_clean.drop('traffic_volume', axis=1)\n","\n","# Extract traffic_volume as target variable\n","y = df_clean['traffic_volume']\n","\n","# Print shapes to verify the split\n","print(\"Features (X) shape:\", X.shape)\n","print(\"Target (y) shape:\", y.shape)\n","\n","# To see the feature names\n","print(\"\\nFeature names:\")\n","print(X.columns.tolist())\n","\n","# To see first few rows of the split\n","print(\"\\nFirst few rows of features (X):\")\n","print(X.head(10))\n","print(\"\\nFirst few rows of target (y):\")\n","print(y.head(10))"],"metadata":{"id":"yX0uBY4VHlR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7F5ap4I2aTYy"},"outputs":[],"source":["# 4b. Data understanding - Make use of plots to build more understanding of the data\n","# Hint: Can use df.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPNxMKp1aTYy"},"outputs":[],"source":["'''\n","# 4b. (Optional) Create More plots to understand the relationship b/w different variables\n","\n","# Example: Top 20 actors of movies based on the imdb rating of the movies\n","\n","plt.figure(figsize=(10, 8))\n","\n","# Create a new dataframe with top 20 values\n","new_df = df.sort_values(by ='imdb_score' , ascending=False)\n","new_df = new_df.head(20)\n","\n","# plotting\n","ax=sns.pointplot(x=new_df['actor_1_name'], y=new_df['imdb_score'], hue=new_df['movie_title'])\n","ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n","plt.tight_layout()\n","plt.show()'''"]},{"cell_type":"code","source":["# 4c. Find out which input features are the most important\n","# Hint: Start out with df.corr(). Can visualise with seaborn library\n","from sklearn.feature_selection import mutual_info_regression, SelectPercentile,SelectKBest\n","# 1. First, explicitly define all features including holiday_encoded\n","features = ['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'hour', 'day',\n","           'month', 'day_of_week', 'weather_main_encoded',\n","           'weather_description_encoded', 'holiday_encoded']  # Explicitly include holiday_encoded\n","\n","# 2. Verify features list\n","print(\"Features to be analyzed:\", features)\n","\n","# 3. Create X and y ensuring holiday_encoded is included\n","X = df_clean[features]  # Features including holiday_encoded\n","y = df_clean['traffic_volume']  # Target\n","\n","# 4. Calculate mutual information\n","mi = mutual_info_regression(X, y)\n","\n","# 5. Create and sort feature importance\n","feature_importance = dict(zip(features, mi))\n","sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n","\n","# 6. Print results ensuring all features are shown\n","print(\"\\nFeature Importance Ranking:\")\n","print(\"-\" * 40)\n","for feature, score in sorted_features:\n","    print(f\"{feature:<25} {score:.4f}\")\n","\n","# 7. Double check holiday_encoded specifically\n","print(\"\\nHoliday encoded importance score:\")\n","print(f\"holiday_encoded: {feature_importance['holiday_encoded']:.4f}\")"],"metadata":{"id":"OIZhABDWfzFN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRJSyJNraTYz"},"source":["### Model Development"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nu0PTJ2SaTYz"},"outputs":[],"source":["# 1. Divide dataframe into input and output\n","# X = df.drop(columns=['output_class']) -> Drop the column to be predicted\n","# y = df['output_class'] -> Choose Output column to be predicted"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"ojckrvZUaTYz","executionInfo":{"status":"ok","timestamp":1735281972104,"user_tz":-330,"elapsed":12655,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}},"outputId":"58894151-a0a9-4434-d639-75b13f497b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["['temp', 'hour', 'day', 'month', 'day_of_week']\n"]}],"source":["# 2. Perform Feature Selection - Experiment with the best one!\n","X = df_clean.drop('traffic_volume', axis=1)\n","X_selected = SelectKBest(mutual_info_regression,k=5).fit_transform(X,y)\n","selected_features = X.columns[selector.get_support()].tolist()\n","print(selected_features)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ZMK_-L-taTYz","executionInfo":{"status":"ok","timestamp":1735282039279,"user_tz":-330,"elapsed":449,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}}},"outputs":[],"source":["# Usually, we do a train-test split, but, in the hackathon, we'll already provide you with the separate datasets for each\n","from sklearn.model_selection import train_test_split\n","# First split your data\n","X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"5xUoyyQXaTYz","executionInfo":{"status":"ok","timestamp":1735282104982,"user_tz":-330,"elapsed":482,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}}},"outputs":[],"source":["# 3. Data Normalisation: Bring into the range 0 to 1, or -1 to 1\n","# StandardScaler is used\n","# weather measurements (temp, rain, snow) likely follow a normal distribution\n","# target (traffic_volume) typically follows a normal distribution\n","# It's most compatible with statistical models and machine learning algorithms\n","from sklearn.preprocessing import StandardScaler\n","# Apply StandardScaler\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"kNFBi9jyk90_","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1735282203106,"user_tz":-330,"elapsed":783,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}},"outputId":"c3404b73-288d-41a6-ef2c-4fdaf7a831c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.13283176730331325\n"]}],"source":["# 4. Choose Model(s), fit\n","### Experiment with different models.\n","### https://scikit-learn.org/stable/supervised_learning.html\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score, mean_squared_error\n","model = LinearRegression()\n","model.fit(X_train_scaled,y_train)\n","y_pred = model.predict(X_test_scaled)\n","print(r2_score(y_test,y_pred))"]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","rf_model = RandomForestRegressor(\n","    n_estimators=100,\n","    max_depth=10,\n","    min_samples_split=5,\n","    min_samples_leaf=2,\n","    random_state=42\n",")\n","rf_model.fit(X_train_scaled, y_train)\n","\n","# 5. Predict and evaluate\n","y_pred = rf_model.predict(X_test_scaled)\n","print(\"R² Score:\", r2_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"VEaeTeDCuj-T","executionInfo":{"status":"ok","timestamp":1735282140262,"user_tz":-330,"elapsed":5992,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}},"outputId":"a4240851-783d-45fa-d13f-fe815f80565f"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["R² Score: 0.7669929336653705\n"]}]},{"cell_type":"markdown","source":["The difference in R² scores between Linear Regression and Random Forest:\n","\n","# ***Linear Regression: R² = 0.1328 (13.28%)***\n","\n","This indicates that the linear model can explain about 13.28% of the variance in traffic volume\n","\n","The low score suggests that the relationship between features and target is not very linear\n","\n","Linear Regression assumes a linear relationship between features and target\n","\n","\n","# ***Random Forest: R² = 0.7669 (76.69%)***\n","\n","Much better performance, explaining about 76.69% of the variance\n","Random Forest performs better because:\n","\n","It can capture non-linear relationships\n","\n","It can handle feature interactions\n","It's more robust to noise in the data\n","It uses multiple decision trees (ensemble method)\n","The parameters we set (n_estimators=100, max_depth=10, etc.) helped optimize the model\n","\n","The significant improvement (from 13% to 77%) suggests that:\n","\n","The relationship between weather features and traffic volume is non-linear\n","\n","There are complex interactions between features\n","Random Forest is more suitable for this type of prediction problem"],"metadata":{"id":"QDaZK6mc5bdD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HzvdnG0mWEZ"},"outputs":[],"source":["# 5. Evaluate with relevant metric for your problem. Eg: accuracy_score(), r2_score()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KYr2JC6aTYz"},"outputs":[],"source":["# 6. After model choice is made, fine-tune with GridSearchCV, or RandomizedSearchCV()\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [10, 15, 20],\n","    'min_samples_split': [2, 5, 10]\n","}\n","grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n","grid_search.fit(X_train_scaled, y_train)"]},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5)\n","print(\"Cross-validation scores:\", scores)"],"metadata":{"id":"WvZ4Ryf6GIe-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Zb-W-KzvyzX"},"source":["### Testing and Creating Output CSV"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"X833FyDYv0IY","executionInfo":{"status":"ok","timestamp":1735285324643,"user_tz":-330,"elapsed":473,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}}},"outputs":[],"source":["# Creating output file for submission - Template Code\n","\n","test_pd = pd.read_csv('/content/test_set_nogt.csv')\n","\n","# saving in a new variable to modify\n","test = test_pd.copy(deep=True)\n","\n","# Prepare data to be given as an input to your trained model\n","# 1. Repeat the pre-processing done above. Eg: Conversion to categorical, filling in mean values\n","\n","# --- Perform the same pre-processing steps as you did for the training data ---\n","# Handle datetime to extract 'hour', 'day', 'month', 'day_of_week'\n","test['date_time'] = pd.to_datetime(test['date_time'], format='%d-%m-%Y %H:%M')\n","test['hour'] = test['date_time'].dt.hour\n","test['day'] = test['date_time'].dt.day\n","test['month'] = test['date_time'].dt.month\n","test['day_of_week'] = test['date_time'].dt.dayofweek\n","\n","# Handle categorical variables using Label Encoding\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","categorical_columns = ['weather_main', 'weather_description', 'holiday']\n","\n","for col in categorical_columns:\n","    test[f'{col}_encoded'] = le.fit_transform(test[col].fillna('Missing'))\n","\n","# Drop original categorical columns\n","columns_to_drop = ['holiday', 'weather_main', 'weather_description', 'date_time']\n","test = test.drop(columns=columns_to_drop)\n","\n","# 2. Use the same features obtained in feature selection\n","chosen_features = selected_features # from above -> getting names of chosen features\n","test = test[chosen_features]\n","\n","# 3. Normalise/Scale the features as done above\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()  # Assuming you used StandardScaler before\n","test = scaler.fit_transform(test)  # Apply scaling\n","\n","# 4. Predict and obtain results from the model\n","y_pred = model.predict(test)\n","\n","# 5. Save results to CSV\n","submission = pd.DataFrame({'ID': test_pd.index, 'Traffic_volume' : y_pred})\n","submission.to_csv('output_submission_eval_Lr.csv', index=False)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"xRKZLwnaoFQf","executionInfo":{"status":"ok","timestamp":1735285416807,"user_tz":-330,"elapsed":2,"user":{"displayName":"Brahmananda B","userId":"11459238854897743545"}}},"outputs":[],"source":["# 4. Predict and obtain results from the model\n","y_pred = model.predict(test)\n","\n","# 5. Save results to CSV\n","submission = pd.DataFrame({'ID': test_pd.index, 'Traffic_volume' : y_pred})\n","submission.to_csv('output_submission_eval_RFF.csv', index=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}